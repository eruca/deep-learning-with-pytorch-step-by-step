{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:cpu\n"
     ]
    }
   ],
   "source": [
    "%run -i data_generation/simple_linear_regression.py\n",
    "%run -i data_preparetion/v0.py\n",
    "%run -i model_configuration/v0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # 构建在训练循环中执行一个步骤的函数\n",
    "    def perform_train_step(x, y):\n",
    "        # 设置模型为训练模式\n",
    "        model.train()\n",
    "\n",
    "        # 第1步：计算模型的预测输出--前向传递\n",
    "        yhat = model(x)\n",
    "        # 第2步：计算损失\n",
    "        loss = loss_fn(yhat, y)\n",
    "        # 第3步：计算参数a和b的梯度\n",
    "        loss.backward()\n",
    "        # 第4步：使用梯度和学习率更新参数\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 返回损失\n",
    "        return loss.item()\n",
    "    \n",
    "    # 返回将在训练循环内调用的函数\n",
    "    return perform_train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:cpu\n"
     ]
    }
   ],
   "source": [
    "%run -i data_preparetion/v0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_configuration/v1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_configuration/v1.py\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 设置学习率\n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# 现在可以创建一个模型并立即将其发送到设备\n",
    "model = nn.Sequential(nn.Linear(1,1)).to(device)\n",
    "\n",
    "# 定义SGD优化器来更新参数\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# 定义MSE损伤函数\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# 为模型、损伤函数和优化器创建train_step函数\n",
    "train_step = make_train_step(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i model_configuration/v1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.make_train_step.<locals>.perform_train_step(x, y)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_training/v1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_training/v1.py\n",
    "\n",
    "# 定义周期数\n",
    "n_epochs = 1000\n",
    "\n",
    "losses = []\n",
    "\n",
    "# 对于每个周期...\n",
    "for epoch in range(n_epochs):\n",
    "    # 执行一个训练步骤并返回相应的损失\n",
    "    loss = train_step(x_train_tensor, y_train_tensor)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i model_training/v1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7255967855453491,\n",
       " 0.4431508183479309,\n",
       " 0.28266122937202454,\n",
       " 0.19111186265945435,\n",
       " 0.13854403793811798,\n",
       " 0.10802850872278214,\n",
       " 0.0899982899427414,\n",
       " 0.07904667407274246,\n",
       " 0.0721178874373436,\n",
       " 0.06748442351818085,\n",
       " 0.0641695037484169,\n",
       " 0.061620552092790604,\n",
       " 0.059524692595005035,\n",
       " 0.057704489678144455,\n",
       " 0.056059159338474274,\n",
       " 0.05453111603856087,\n",
       " 0.05308728665113449,\n",
       " 0.051708467304706573,\n",
       " 0.050383228808641434,\n",
       " 0.04910467937588692,\n",
       " 0.04786836355924606,\n",
       " 0.046671345829963684,\n",
       " 0.04551146179437637,\n",
       " 0.044387079775333405,\n",
       " 0.04329680651426315,\n",
       " 0.04223943501710892,\n",
       " 0.04121391102671623,\n",
       " 0.04021921381354332,\n",
       " 0.03925437480211258,\n",
       " 0.03831850737333298,\n",
       " 0.03741069883108139,\n",
       " 0.03653012588620186,\n",
       " 0.035675954073667526,\n",
       " 0.03484740108251572,\n",
       " 0.034043699502944946,\n",
       " 0.03326408565044403,\n",
       " 0.032507866621017456,\n",
       " 0.031774312257766724,\n",
       " 0.031062761321663857,\n",
       " 0.03037254884839058,\n",
       " 0.029703030362725258,\n",
       " 0.02905358001589775,\n",
       " 0.0284236129373312,\n",
       " 0.027812525629997253,\n",
       " 0.02721976675093174,\n",
       " 0.026644786819815636,\n",
       " 0.026087045669555664,\n",
       " 0.025546032935380936,\n",
       " 0.025021236389875412,\n",
       " 0.024512173607945442,\n",
       " 0.02401837706565857,\n",
       " 0.02353939600288868,\n",
       " 0.02307477593421936,\n",
       " 0.022624081000685692,\n",
       " 0.022186901420354843,\n",
       " 0.02176283672451973,\n",
       " 0.02135148085653782,\n",
       " 0.020952466875314713,\n",
       " 0.02056541107594967,\n",
       " 0.02018996700644493,\n",
       " 0.019825782626867294,\n",
       " 0.019472509622573853,\n",
       " 0.019129838794469833,\n",
       " 0.018797438591718674,\n",
       " 0.018475009128451347,\n",
       " 0.01816224679350853,\n",
       " 0.01785886473953724,\n",
       " 0.017564574256539345,\n",
       " 0.017279107123613358,\n",
       " 0.017002202570438385,\n",
       " 0.016733605414628983,\n",
       " 0.01647305302321911,\n",
       " 0.016220321878790855,\n",
       " 0.015975167974829674,\n",
       " 0.015737362205982208,\n",
       " 0.015506690368056297,\n",
       " 0.015282933600246906,\n",
       " 0.015065890736877918,\n",
       " 0.014855345711112022,\n",
       " 0.014651131816208363,\n",
       " 0.01445302926003933,\n",
       " 0.014260867610573769,\n",
       " 0.014074468985199928,\n",
       " 0.01389365829527378,\n",
       " 0.013718271628022194,\n",
       " 0.013548141345381737,\n",
       " 0.013383114710450172,\n",
       " 0.013223035261034966,\n",
       " 0.013067761436104774,\n",
       " 0.012917136773467064,\n",
       " 0.012771029956638813,\n",
       " 0.012629309669137001,\n",
       " 0.012491835281252861,\n",
       " 0.01235848106443882,\n",
       " 0.012229127809405327,\n",
       " 0.012103654444217682,\n",
       " 0.01198194082826376,\n",
       " 0.011863874271512032,\n",
       " 0.01174935419112444,\n",
       " 0.011638259515166283,\n",
       " 0.011530505493283272,\n",
       " 0.011425980366766453,\n",
       " 0.011324587278068066,\n",
       " 0.011226235888898373,\n",
       " 0.011130832135677338,\n",
       " 0.011038290336728096,\n",
       " 0.010948525741696358,\n",
       " 0.01086144708096981,\n",
       " 0.010776986368000507,\n",
       " 0.010695051401853561,\n",
       " 0.010615577921271324,\n",
       " 0.010538486763834953,\n",
       " 0.010463709011673927,\n",
       " 0.010391170158982277,\n",
       " 0.010320807807147503,\n",
       " 0.010252555832266808,\n",
       " 0.010186350904405117,\n",
       " 0.010122130624949932,\n",
       " 0.010059838183224201,\n",
       " 0.009999407455325127,\n",
       " 0.009940793737769127,\n",
       " 0.009883936494588852,\n",
       " 0.009828782640397549,\n",
       " 0.009775286540389061,\n",
       " 0.009723390452563763,\n",
       " 0.009673049673438072,\n",
       " 0.009624222293496132,\n",
       " 0.009576857089996338,\n",
       " 0.009530912153422832,\n",
       " 0.009486344642937183,\n",
       " 0.009443113580346107,\n",
       " 0.009401179850101471,\n",
       " 0.00936050433665514,\n",
       " 0.009321046061813831,\n",
       " 0.009282774291932583,\n",
       " 0.009245648980140686,\n",
       " 0.009209634736180305,\n",
       " 0.009174702689051628,\n",
       " 0.009140814654529095,\n",
       " 0.00910794734954834,\n",
       " 0.00907606165856123,\n",
       " 0.009045137092471123,\n",
       " 0.009015137329697609,\n",
       " 0.008986036293208599,\n",
       " 0.008957806043326855,\n",
       " 0.008930424228310585,\n",
       " 0.008903863839805126,\n",
       " 0.008878102526068687,\n",
       " 0.00885311234742403,\n",
       " 0.008828869089484215,\n",
       " 0.008805353194475174,\n",
       " 0.008782541379332542,\n",
       " 0.008760417811572552,\n",
       " 0.008738955482840538,\n",
       " 0.008718134835362434,\n",
       " 0.008697941899299622,\n",
       " 0.00867835246026516,\n",
       " 0.008659349754452705,\n",
       " 0.008640916086733341,\n",
       " 0.008623038418591022,\n",
       " 0.008605694398283958,\n",
       " 0.008588874712586403,\n",
       " 0.00857255607843399,\n",
       " 0.008556725457310677,\n",
       " 0.008541369810700417,\n",
       " 0.008526472374796867,\n",
       " 0.008512025699019432,\n",
       " 0.00849801767617464,\n",
       " 0.00848442129790783,\n",
       " 0.008471233770251274,\n",
       " 0.008458441123366356,\n",
       " 0.008446035906672478,\n",
       " 0.008433999493718147,\n",
       " 0.008422323502600193,\n",
       " 0.008410998620092869,\n",
       " 0.008400014601647854,\n",
       " 0.008389359340071678,\n",
       " 0.008379021659493446,\n",
       " 0.00836899597197771,\n",
       " 0.008359270170331001,\n",
       " 0.008349834010004997,\n",
       " 0.008340688422322273,\n",
       " 0.008331811055541039,\n",
       " 0.00832319725304842,\n",
       " 0.008314842358231544,\n",
       " 0.008306743577122688,\n",
       " 0.008298882283270359,\n",
       " 0.008291258476674557,\n",
       " 0.008283866569399834,\n",
       " 0.008276691660284996,\n",
       " 0.008269735611975193,\n",
       " 0.008262988179922104,\n",
       " 0.008256440982222557,\n",
       " 0.00825008936226368,\n",
       " 0.008243930526077747,\n",
       " 0.00823795422911644,\n",
       " 0.008232155814766884,\n",
       " 0.008226536214351654,\n",
       " 0.008221079595386982,\n",
       " 0.00821579061448574,\n",
       " 0.008210658095777035,\n",
       " 0.008205684833228588,\n",
       " 0.008200853131711483,\n",
       " 0.008196169510483742,\n",
       " 0.008191627450287342,\n",
       " 0.008187221363186836,\n",
       " 0.008182944729924202,\n",
       " 0.008178796619176865,\n",
       " 0.008174775168299675,\n",
       " 0.00817087758332491,\n",
       " 0.00816708616912365,\n",
       " 0.008163420483469963,\n",
       " 0.00815985631197691,\n",
       " 0.00815640203654766,\n",
       " 0.008153053931891918,\n",
       " 0.008149800822138786,\n",
       " 0.008146647363901138,\n",
       " 0.008143587969243526,\n",
       " 0.008140623569488525,\n",
       " 0.008137745782732964,\n",
       " 0.008134951815009117,\n",
       " 0.008132247254252434,\n",
       " 0.008129619061946869,\n",
       " 0.008127073757350445,\n",
       " 0.008124599233269691,\n",
       " 0.00812220387160778,\n",
       " 0.008119876496493816,\n",
       " 0.008117621764540672,\n",
       " 0.0081154340878129,\n",
       " 0.008113311603665352,\n",
       " 0.008111255243420601,\n",
       " 0.008109256625175476,\n",
       " 0.008107317611575127,\n",
       " 0.008105440065264702,\n",
       " 0.008103619329631329,\n",
       " 0.008101848885416985,\n",
       " 0.008100135251879692,\n",
       " 0.008098472841084003,\n",
       " 0.008096858859062195,\n",
       " 0.008095292374491692,\n",
       " 0.008093775250017643,\n",
       " 0.008092300966382027,\n",
       " 0.008090869523584843,\n",
       " 0.008089486509561539,\n",
       " 0.008088143542408943,\n",
       " 0.008086837828159332,\n",
       " 0.008085573092103004,\n",
       " 0.008084346540272236,\n",
       " 0.008083155378699303,\n",
       " 0.008082003332674503,\n",
       " 0.008080879226326942,\n",
       " 0.00807979516685009,\n",
       " 0.00807874370366335,\n",
       " 0.008077718317508698,\n",
       " 0.008076729252934456,\n",
       " 0.008075766265392303,\n",
       " 0.008074834942817688,\n",
       " 0.008073928765952587,\n",
       " 0.008073054254055023,\n",
       " 0.008072199299931526,\n",
       " 0.008071373216807842,\n",
       " 0.008070572279393673,\n",
       " 0.008069795556366444,\n",
       " 0.008069043979048729,\n",
       " 0.008068312890827656,\n",
       " 0.008067603223025799,\n",
       " 0.008066912181675434,\n",
       " 0.008066244423389435,\n",
       " 0.008065599016845226,\n",
       " 0.008064971305429935,\n",
       " 0.008064361289143562,\n",
       " 0.00806377176195383,\n",
       " 0.008063197135925293,\n",
       " 0.008062641136348248,\n",
       " 0.008062101900577545,\n",
       " 0.008061576634645462,\n",
       " 0.008061069063842297,\n",
       " 0.008060579188168049,\n",
       " 0.008060104213654995,\n",
       " 0.008059638552367687,\n",
       " 0.008059188723564148,\n",
       " 0.008058753795921803,\n",
       " 0.008058330044150352,\n",
       " 0.008057919330894947,\n",
       " 0.008057521656155586,\n",
       " 0.00805713888257742,\n",
       " 0.00805676355957985,\n",
       " 0.00805640034377575,\n",
       " 0.008056048303842545,\n",
       " 0.008055704645812511,\n",
       " 0.008055375888943672,\n",
       " 0.008055053651332855,\n",
       " 0.008054741658270359,\n",
       " 0.008054441772401333,\n",
       " 0.008054148405790329,\n",
       " 0.008053859695792198,\n",
       " 0.00805358774960041,\n",
       " 0.008053319528698921,\n",
       " 0.00805305689573288,\n",
       " 0.008052805438637733,\n",
       " 0.008052565157413483,\n",
       " 0.008052324876189232,\n",
       " 0.008052095770835876,\n",
       " 0.00805187039077282,\n",
       " 0.008051656186580658,\n",
       " 0.00805144477635622,\n",
       " 0.008051241748034954,\n",
       " 0.008051043376326561,\n",
       " 0.008050852455198765,\n",
       " 0.008050664328038692,\n",
       " 0.008050484582781792,\n",
       " 0.008050312288105488,\n",
       " 0.008050141856074333,\n",
       " 0.0080499779433012,\n",
       " 0.008049819618463516,\n",
       " 0.008049664087593555,\n",
       " 0.008049512282013893,\n",
       " 0.008049366995692253,\n",
       " 0.008049228228628635,\n",
       " 0.008049088530242443,\n",
       " 0.008048956282436848,\n",
       " 0.0080488296225667,\n",
       " 0.008048705756664276,\n",
       " 0.008048579096794128,\n",
       " 0.008048463612794876,\n",
       " 0.008048350922763348,\n",
       " 0.008048237301409245,\n",
       " 0.008048130199313164,\n",
       " 0.008048027753829956,\n",
       " 0.008047926239669323,\n",
       " 0.008047828450798988,\n",
       " 0.008047732524573803,\n",
       " 0.00804764125496149,\n",
       " 0.008047551847994328,\n",
       " 0.008047464303672314,\n",
       " 0.008047379553318024,\n",
       " 0.008047297596931458,\n",
       " 0.00804721750319004,\n",
       " 0.008047141134738922,\n",
       " 0.008047069422900677,\n",
       " 0.008046995848417282,\n",
       " 0.00804692693054676,\n",
       " 0.00804685615003109,\n",
       " 0.008046790957450867,\n",
       " 0.008046727627515793,\n",
       " 0.008046667091548443,\n",
       " 0.008046604692935944,\n",
       " 0.008046545088291168,\n",
       " 0.008046488277614117,\n",
       " 0.008046435192227364,\n",
       " 0.00804638210684061,\n",
       " 0.00804633367806673,\n",
       " 0.008046279661357403,\n",
       " 0.008046228438615799,\n",
       " 0.008046183735132217,\n",
       " 0.008046138100326061,\n",
       " 0.008046094328165054,\n",
       " 0.008046050556004047,\n",
       " 0.008046010509133339,\n",
       " 0.008045969530940056,\n",
       " 0.008045931346714497,\n",
       " 0.008045889437198639,\n",
       " 0.008045854978263378,\n",
       " 0.008045816794037819,\n",
       " 0.008045784197747707,\n",
       " 0.008045749738812447,\n",
       " 0.008045715279877186,\n",
       " 0.008045683614909649,\n",
       " 0.008045653812587261,\n",
       " 0.008045625872910023,\n",
       " 0.008045594207942486,\n",
       " 0.008045566268265247,\n",
       " 0.008045537397265434,\n",
       " 0.008045515045523643,\n",
       " 0.00804548617452383,\n",
       " 0.008045462891459465,\n",
       " 0.00804543774574995,\n",
       " 0.008045416325330734,\n",
       " 0.008045393042266369,\n",
       " 0.008045369759202003,\n",
       " 0.008045347407460213,\n",
       " 0.00804532878100872,\n",
       " 0.008045310154557228,\n",
       " 0.008045287802815437,\n",
       " 0.00804527010768652,\n",
       " 0.008045251481235027,\n",
       " 0.00804523192346096,\n",
       " 0.008045214228332043,\n",
       " 0.008045198395848274,\n",
       " 0.008045181632041931,\n",
       " 0.008045164868235588,\n",
       " 0.00804514903575182,\n",
       " 0.008045134134590626,\n",
       " 0.00804512295871973,\n",
       " 0.008045107126235962,\n",
       " 0.008045095019042492,\n",
       " 0.008045080117881298,\n",
       " 0.008045069873332977,\n",
       " 0.008045054040849209,\n",
       " 0.008045045658946037,\n",
       " 0.008045034483075142,\n",
       " 0.008045020513236523,\n",
       " 0.008045010268688202,\n",
       " 0.008045000024139881,\n",
       " 0.00804498977959156,\n",
       " 0.00804497767239809,\n",
       " 0.008044971153140068,\n",
       " 0.008044959977269173,\n",
       " 0.008044950664043427,\n",
       " 0.00804494135081768,\n",
       " 0.008044932037591934,\n",
       " 0.008044925518333912,\n",
       " 0.008044918067753315,\n",
       " 0.008044909685850143,\n",
       " 0.008044902235269547,\n",
       " 0.008044893853366375,\n",
       " 0.008044888265430927,\n",
       " 0.008044878952205181,\n",
       " 0.008044873364269733,\n",
       " 0.008044866845011711,\n",
       " 0.008044862188398838,\n",
       " 0.008044853806495667,\n",
       " 0.008044849149882793,\n",
       " 0.008044843561947346,\n",
       " 0.008044839836657047,\n",
       " 0.008044828660786152,\n",
       " 0.008044825866818428,\n",
       " 0.00804482214152813,\n",
       " 0.008044817484915257,\n",
       " 0.008044811896979809,\n",
       " 0.008044807240366936,\n",
       " 0.008044803515076637,\n",
       " 0.008044796995818615,\n",
       " 0.008044792339205742,\n",
       " 0.008044792339205742,\n",
       " 0.008044786751270294,\n",
       " 0.008044782094657421,\n",
       " 0.008044780232012272,\n",
       " 0.008044775575399399,\n",
       " 0.00804477371275425,\n",
       " 0.008044767193496227,\n",
       " 0.008044766262173653,\n",
       " 0.00804476160556078,\n",
       " 0.008044757880270481,\n",
       " 0.008044755086302757,\n",
       " 0.008044751361012459,\n",
       " 0.008044748567044735,\n",
       " 0.00804474763572216,\n",
       " 0.008044744841754436,\n",
       " 0.008044741116464138,\n",
       " 0.008044738322496414,\n",
       " 0.00804473552852869,\n",
       " 0.008044732734560966,\n",
       " 0.008044730871915817,\n",
       " 0.008044727146625519,\n",
       " 0.008044729009270668,\n",
       " 0.008044724352657795,\n",
       " 0.008044722490012646,\n",
       " 0.008044722490012646,\n",
       " 0.008044720627367496,\n",
       " 0.008044717833399773,\n",
       " 0.008044716902077198,\n",
       " 0.008044712245464325,\n",
       " 0.008044714108109474,\n",
       " 0.008044708520174026,\n",
       " 0.008044710382819176,\n",
       " 0.008044709451496601,\n",
       " 0.008044702932238579,\n",
       " 0.008044702932238579,\n",
       " 0.00804470106959343,\n",
       " 0.00804470106959343,\n",
       " 0.00804469920694828,\n",
       " 0.00804469920694828,\n",
       " 0.00804469920694828,\n",
       " 0.008044694550335407,\n",
       " 0.008044692687690258,\n",
       " 0.008044694550335407,\n",
       " 0.008044691756367683,\n",
       " 0.008044688031077385,\n",
       " 0.008044691756367683,\n",
       " 0.008044688031077385,\n",
       " 0.008044688031077385,\n",
       " 0.008044688031077385,\n",
       " 0.00804468709975481,\n",
       " 0.008044686168432236,\n",
       " 0.008044685237109661,\n",
       " 0.008044682443141937,\n",
       " 0.008044684305787086,\n",
       " 0.008044684305787086,\n",
       " 0.008044680580496788,\n",
       " 0.008044681511819363,\n",
       " 0.008044684305787086,\n",
       " 0.008044679649174213,\n",
       " 0.008044677786529064,\n",
       " 0.008044677786529064,\n",
       " 0.00804467685520649,\n",
       " 0.008044673129916191,\n",
       " 0.00804467685520649,\n",
       " 0.00804467685520649,\n",
       " 0.008044675923883915,\n",
       " 0.008044674061238766,\n",
       " 0.00804467685520649,\n",
       " 0.008044674061238766,\n",
       " 0.008044672198593616,\n",
       " 0.008044673129916191,\n",
       " 0.008044672198593616,\n",
       " 0.008044673129916191,\n",
       " 0.008044673129916191,\n",
       " 0.008044672198593616,\n",
       " 0.008044671267271042,\n",
       " 0.008044669404625893,\n",
       " 0.008044670335948467,\n",
       " 0.008044668473303318,\n",
       " 0.008044669404625893,\n",
       " 0.008044666610658169,\n",
       " 0.008044666610658169,\n",
       " 0.008044665679335594,\n",
       " 0.008044667541980743,\n",
       " 0.008044665679335594,\n",
       " 0.008044665679335594,\n",
       " 0.008044665679335594,\n",
       " 0.00804466288536787,\n",
       " 0.008044665679335594,\n",
       " 0.008044665679335594,\n",
       " 0.00804466474801302,\n",
       " 0.008044663816690445,\n",
       " 0.00804466474801302,\n",
       " 0.00804466474801302,\n",
       " 0.008044665679335594,\n",
       " 0.00804466288536787,\n",
       " 0.008044661954045296,\n",
       " 0.00804466288536787,\n",
       " 0.008044661954045296,\n",
       " 0.008044663816690445,\n",
       " 0.008044663816690445,\n",
       " 0.008044665679335594,\n",
       " 0.00804466288536787,\n",
       " 0.00804466288536787,\n",
       " 0.00804466474801302,\n",
       " 0.008044661954045296,\n",
       " 0.008044660091400146,\n",
       " 0.008044661954045296,\n",
       " 0.008044661954045296,\n",
       " 0.008044658228754997,\n",
       " 0.008044661954045296,\n",
       " 0.008044660091400146,\n",
       " 0.008044661022722721,\n",
       " 0.008044660091400146,\n",
       " 0.008044661022722721,\n",
       " 0.008044659160077572,\n",
       " 0.008044660091400146,\n",
       " 0.008044661022722721,\n",
       " 0.008044661022722721,\n",
       " 0.008044660091400146,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044661022722721,\n",
       " 0.008044661954045296,\n",
       " 0.008044659160077572,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044659160077572,\n",
       " 0.008044657297432423,\n",
       " 0.008044657297432423,\n",
       " 0.008044658228754997,\n",
       " 0.008044661954045296,\n",
       " 0.008044658228754997,\n",
       " 0.008044659160077572,\n",
       " 0.008044658228754997,\n",
       " 0.008044659160077572,\n",
       " 0.008044658228754997,\n",
       " 0.008044657297432423,\n",
       " 0.008044657297432423,\n",
       " 0.008044658228754997,\n",
       " 0.008044656366109848,\n",
       " 0.008044658228754997,\n",
       " 0.008044656366109848,\n",
       " 0.008044657297432423,\n",
       " 0.008044661022722721,\n",
       " 0.008044659160077572,\n",
       " 0.008044657297432423,\n",
       " 0.008044657297432423,\n",
       " 0.008044659160077572,\n",
       " 0.008044658228754997,\n",
       " 0.008044657297432423,\n",
       " 0.008044658228754997,\n",
       " 0.008044661954045296,\n",
       " 0.008044657297432423,\n",
       " 0.008044657297432423,\n",
       " 0.008044658228754997,\n",
       " 0.008044657297432423,\n",
       " 0.008044657297432423,\n",
       " 0.008044658228754997,\n",
       " 0.008044659160077572,\n",
       " 0.008044657297432423,\n",
       " 0.008044655434787273,\n",
       " 0.008044660091400146,\n",
       " 0.008044657297432423,\n",
       " 0.008044657297432423,\n",
       " 0.008044657297432423,\n",
       " 0.008044658228754997,\n",
       " 0.008044657297432423,\n",
       " 0.008044655434787273,\n",
       " 0.008044657297432423,\n",
       " 0.008044656366109848,\n",
       " 0.008044657297432423,\n",
       " 0.008044657297432423,\n",
       " 0.008044657297432423,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044655434787273,\n",
       " 0.008044657297432423,\n",
       " 0.008044655434787273,\n",
       " 0.008044658228754997,\n",
       " 0.008044654503464699,\n",
       " 0.008044657297432423,\n",
       " 0.008044654503464699,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044655434787273,\n",
       " 0.008044656366109848,\n",
       " 0.008044655434787273,\n",
       " 0.008044658228754997,\n",
       " 0.008044655434787273,\n",
       " 0.008044656366109848,\n",
       " 0.008044655434787273,\n",
       " 0.008044657297432423,\n",
       " 0.008044657297432423,\n",
       " 0.008044656366109848,\n",
       " 0.008044658228754997,\n",
       " 0.008044654503464699,\n",
       " 0.008044656366109848,\n",
       " 0.008044655434787273,\n",
       " 0.008044657297432423,\n",
       " 0.008044653572142124,\n",
       " 0.008044658228754997,\n",
       " 0.008044661022722721,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044656366109848,\n",
       " 0.008044656366109848,\n",
       " 0.008044659160077572,\n",
       " 0.008044658228754997,\n",
       " 0.008044654503464699,\n",
       " 0.008044658228754997,\n",
       " 0.008044654503464699,\n",
       " 0.008044657297432423,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044656366109848,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044655434787273,\n",
       " 0.008044654503464699,\n",
       " 0.008044655434787273,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044655434787273,\n",
       " 0.008044658228754997,\n",
       " 0.008044657297432423,\n",
       " 0.008044658228754997,\n",
       " 0.008044657297432423,\n",
       " 0.008044655434787273,\n",
       " 0.008044657297432423,\n",
       " 0.008044655434787273,\n",
       " 0.008044656366109848,\n",
       " 0.008044656366109848,\n",
       " 0.008044655434787273,\n",
       " 0.008044658228754997,\n",
       " 0.008044654503464699,\n",
       " 0.008044654503464699,\n",
       " 0.008044657297432423,\n",
       " 0.008044657297432423,\n",
       " 0.008044655434787273,\n",
       " 0.008044658228754997,\n",
       " 0.008044657297432423,\n",
       " 0.008044659160077572,\n",
       " 0.008044655434787273,\n",
       " 0.008044657297432423,\n",
       " 0.008044655434787273,\n",
       " 0.008044654503464699,\n",
       " 0.008044654503464699,\n",
       " 0.008044656366109848,\n",
       " 0.008044656366109848,\n",
       " 0.008044653572142124,\n",
       " 0.008044655434787273,\n",
       " 0.008044658228754997,\n",
       " 0.008044656366109848,\n",
       " 0.008044655434787273,\n",
       " 0.008044658228754997,\n",
       " 0.008044660091400146,\n",
       " 0.008044654503464699,\n",
       " 0.008044654503464699,\n",
       " 0.008044658228754997,\n",
       " 0.008044657297432423,\n",
       " 0.008044655434787273,\n",
       " 0.008044657297432423,\n",
       " 0.008044658228754997,\n",
       " 0.008044655434787273,\n",
       " 0.008044656366109848,\n",
       " 0.008044660091400146,\n",
       " 0.008044658228754997,\n",
       " 0.008044654503464699,\n",
       " 0.008044656366109848,\n",
       " 0.008044654503464699,\n",
       " 0.008044654503464699,\n",
       " 0.008044654503464699,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044658228754997,\n",
       " 0.008044657297432423,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044657297432423,\n",
       " 0.008044658228754997,\n",
       " 0.008044654503464699,\n",
       " 0.008044655434787273,\n",
       " 0.008044657297432423,\n",
       " 0.008044658228754997,\n",
       " 0.008044656366109848,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044656366109848,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044657297432423,\n",
       " 0.008044659160077572,\n",
       " 0.008044657297432423,\n",
       " 0.008044657297432423,\n",
       " 0.008044657297432423,\n",
       " 0.008044655434787273,\n",
       " 0.008044654503464699,\n",
       " 0.008044656366109848,\n",
       " 0.008044657297432423,\n",
       " 0.008044657297432423,\n",
       " 0.008044655434787273,\n",
       " 0.008044657297432423,\n",
       " 0.008044657297432423,\n",
       " 0.008044654503464699,\n",
       " 0.008044655434787273,\n",
       " 0.008044656366109848,\n",
       " 0.008044658228754997,\n",
       " 0.008044655434787273,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044654503464699,\n",
       " 0.008044658228754997,\n",
       " 0.008044654503464699,\n",
       " 0.008044657297432423,\n",
       " 0.008044656366109848,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044657297432423,\n",
       " 0.008044657297432423,\n",
       " 0.008044658228754997,\n",
       " 0.008044656366109848,\n",
       " 0.008044658228754997,\n",
       " 0.008044656366109848,\n",
       " 0.008044654503464699,\n",
       " 0.008044654503464699,\n",
       " 0.008044655434787273,\n",
       " 0.008044656366109848,\n",
       " 0.008044656366109848,\n",
       " 0.008044656366109848,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044656366109848,\n",
       " 0.008044656366109848,\n",
       " 0.008044657297432423,\n",
       " 0.008044654503464699,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044658228754997,\n",
       " 0.008044657297432423,\n",
       " 0.008044656366109848,\n",
       " 0.008044657297432423,\n",
       " 0.008044655434787273,\n",
       " 0.008044657297432423,\n",
       " 0.008044654503464699,\n",
       " 0.008044654503464699,\n",
       " 0.008044654503464699,\n",
       " 0.008044654503464699,\n",
       " 0.008044654503464699,\n",
       " 0.008044657297432423,\n",
       " 0.008044656366109848,\n",
       " 0.008044655434787273,\n",
       " 0.008044656366109848,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273,\n",
       " 0.008044655434787273]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.9690]])), ('0.bias', tensor([1.0235]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.7713]), tensor([2.4745]))\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        super().__init__()\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.7713]), tensor([2.4745]))\n"
     ]
    }
   ],
   "source": [
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.2809],\n",
       "         [0.3253],\n",
       "         [0.1560],\n",
       "         [0.5924],\n",
       "         [0.0651],\n",
       "         [0.8872],\n",
       "         [0.4938],\n",
       "         [0.0055],\n",
       "         [0.1409],\n",
       "         [0.0885],\n",
       "         [0.1849],\n",
       "         [0.7290],\n",
       "         [0.8662],\n",
       "         [0.3117],\n",
       "         [0.6842],\n",
       "         [0.1987]]),\n",
       " tensor([[1.5846],\n",
       "         [1.8057],\n",
       "         [1.2901],\n",
       "         [2.1687],\n",
       "         [1.1559],\n",
       "         [2.8708],\n",
       "         [1.9060],\n",
       "         [1.0632],\n",
       "         [1.1211],\n",
       "         [1.0708],\n",
       "         [1.5888],\n",
       "         [2.4927],\n",
       "         [2.6805],\n",
       "         [1.7637],\n",
       "         [2.3492],\n",
       "         [1.2654]])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_preparetion/v1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_preparetion/v1.py\n",
    "\n",
    "# 数据在numpy数组中\n",
    "# 但需要将它们转化为PyTorch的张量\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "# 构建Dataset\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "\n",
    "# 构建DataLoader\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i data_preparetion/v1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_training/v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_training/v2.py\n",
    "\n",
    "# 定义周期数\n",
    "n_epochs = 1000\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # 内循环\n",
    "    mini_batch_losses = []\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # 数据集存在于CPU中，小批量也是如此\n",
    "        # 因此需要将这些小批量，发送到模型存在的设备\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # 执行一个训练步骤\n",
    "        # 并返回小批量的相应损失\n",
    "        mini_batch_loss = train_step(x_batch, y_batch)\n",
    "        mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "    # 计算所有小批量的平均损失-- 这是周期损失\n",
    "    loss = np.mean(mini_batch_losses)\n",
    "\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i model_training/v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.9698]])), ('0.bias', tensor([1.0246]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i data_preparetion/v1.py\n",
    "%run -i model_configuration/v1.py\n",
    "%run -i model_training/v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch(device, data_loader, step):\n",
    "    mini_batch_losses = []\n",
    "    for x_batch, y_batch in data_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        mini_batch_loss = step(x_batch, y_batch)\n",
    "        mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "    loss = np.mean(mini_batch_losses)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_training/v3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_training/v3.py\n",
    "\n",
    "# 定义周期数\n",
    "n_epochs = 200\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss = mini_batch(device, train_loader, train_step)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i data_preparetion/v1.py\n",
    "%run -i model_configuration/v1.py\n",
    "%run -i model_training/v3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.9684]])), ('0.bias', tensor([1.0219]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_preparetion/v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_preparetion/v2.py\n",
    "\n",
    "torch.manual_seed(13)\n",
    "\n",
    "# 在拆分之前从numpy数组构建张量\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "# 构建包含所有数据点的数据集\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# 执行拆分\n",
    "ratio = .8\n",
    "n_total = len(dataset)\n",
    "n_train = int(n_total * ratio)\n",
    "n_val = n_total - n_train\n",
    "\n",
    "train_data, val_data = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "# 构建每个集合的加载器\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i data_preparetion/v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_val_step(model, loss_fn):\n",
    "    # 在验证循环中构建执行步骤的函数\n",
    "    def perform_val_step(x, y):\n",
    "        # 设置模型为评估模式\n",
    "        model.eval()\n",
    "\n",
    "        # 第1步：计算模型的预测输出--前向传递\n",
    "        yhat = model(x)\n",
    "        # 第2步：计算损失\n",
    "        loss = loss_fn(yhat, y)\n",
    "        # 因为评估期间不更新参数\n",
    "        # 所以无需技术第3步和第4步\n",
    "        return loss.item()\n",
    "    \n",
    "    return perform_val_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_configuration/v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_configuration/v2.py\n",
    "\n",
    "device = 'cude' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 设置学习率\n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# 现在可以创建模型并立即发送到设备\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "\n",
    "# 定义GSD优化器来更新\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# 定义MSE损失函数\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# 为模型、损伤函和优化器创建train_step函数\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "# 为模型和损失函数创建val_step函数\n",
    "val_step = make_val_step(model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i model_configuration/v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_training/v4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_training/v4.py\n",
    "\n",
    "# 定义周期数\n",
    "n_epochs = 200\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # 内循环\n",
    "    loss = mini_batch(device, train_loader, train_step)\n",
    "    losses.append(loss)\n",
    "\n",
    "    # 验证--验证中没有梯度\n",
    "    with torch.no_grad():\n",
    "        val_loss = mini_batch(device, val_loader, val_step)\n",
    "        val_losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i model_training/v4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.9438]])), ('0.bias', tensor([1.0287]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i data_preparetion/v2.py\n",
    "%run -i model_configuration/v2.py\n",
    "%run -i model_training/v4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Tensorboard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取特征(dummy_x)和标签(dummy_y)的元组\n",
    "dummy_x, dummy_y = next(iter(train_loader))\n",
    "\n",
    "# 由于模型已发送到设备，因此需要对数据执行相同的操作\n",
    "# 即使在这里，模型和数据也需要再同一台设备上\n",
    "writer.add_graph(model, dummy_x.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_scalars(\n",
    "    main_tag=\"loss\",\n",
    "    tag_scalar_dict = {\"training\":loss, 'validation': val_loss},\n",
    "    global_step=epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i data_preparetion/v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_configuration/v3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_configuration/v3.py\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 设置学习率\n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "train_step_fn = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "val_step_fn = make_val_step(model, loss_fn)\n",
    "\n",
    "writer = SummaryWriter('runs/simple_linear_regression')\n",
    "\n",
    "# 获取单个小样本，一遍可以使用add_graph\n",
    "x_sample, y_sample = next(iter(train_loader))\n",
    "writer.add_graph(model, x_sample.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i model_configuration/v3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_training/v5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_training/v5.py\n",
    "\n",
    "# 定义周期数\n",
    "n_epochs = 200\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss = mini_batch(device, train_loader, train_step_fn)\n",
    "    losses.append(loss)\n",
    "\n",
    "    # 验证\n",
    "    with torch.no_grad():\n",
    "        val_loss = mini_batch(device, val_loader, val_step_fn)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    # 在主标签“损失”下记录每个周期的损失\n",
    "    writer.add_scalars(\n",
    "        main_tag='loss',\n",
    "        tag_scalar_dict={'training':loss, 'validation':val_loss},\n",
    "        global_step = epoch\n",
    "    )\n",
    "\n",
    "# 关闭编写器\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i model_training/v5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.9432]])), ('0.bias', tensor([1.0263]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "checkpoint = {\n",
    "    'epoch': n_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    'val_loss': val_loss,\n",
    "}\n",
    "torch.save(checkpoint, 'model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i data_preparetion/v2.py\n",
    "%run -i model_configuration/v3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[0.7645]])), ('0.bias', tensor([0.8300]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('model_checkpoint.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "saved_epoch = checkpoint['epoch']\n",
    "save_losses = checkpoint['loss']\n",
    "save_val_losses = checkpoint['val_loss']\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.9411]])), ('0.bias', tensor([1.0230]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i model_training/v5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.9432]])), ('0.bias', tensor([1.0263]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
